{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b162e47a-22d7-4b1d-9521-adb0a8913ce0",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------\n",
    "<font size=6 color=\"#36454F\"><b>Coding Sample for Crogl Team Members Notebook</b></font><br><br>\n",
    "by: Ted Summey\n",
    "\n",
    "This doc exemplifi es a problem that Crogl dev teams come across routinely. If this is the kind of work you enjoy doing and you excel at it, you will love working with the Crogl team.\n",
    "\n",
    "**Problem Statement**\n",
    "\n",
    "As a software developer I want to know if the code that I am using is vulnerable to known vulnerabilities. I am building an application that gathers vulnerability data, analyzes my code and identifi es vulnerabilities in my code. I want a teammate to help me with part of this project.\n",
    "\n",
    "I want you to create a script that downloads all github security vulnerabilities from the [Github Advisory Database](https://github.com/advisories?query=type%3Areviewed+ecosystem%3Apip). Then zips up the advisories by severity: 4 zips for each category of severity: low, moderate, high, critical.\n",
    "\n",
    "Extra credit for generating a csv fi le with a row for every vulnerability and a set of attributes summarizing the key information for each vulnerability. Extra credit for CSV to contain a fi eld called KEV. If the vulnerability is in the CISA Known Exploited Vulnerabilities Catalog, the KEV fi eld value will be 1, Otherwise the fi eld will be empty.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e009c0-8e1d-4f7e-8df4-a7de1b28aed0",
   "metadata": {},
   "source": [
    "![alt Process Flow Diagram](images/flow-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a87c8b-9e92-4c5a-816d-03dd248df611",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size=4 color=\"#36454F\"><b>Code Block #1: Get the number of pages</b></font><br><br>\n",
    "The URL \"https://github.com/advisories?query=type%3Areviewed+ecosystem%3Apip\" (GitHub Advisory Database) is actually a URL with search parameters applied, base URL https://github.com/advisories with type:reviewed ecosystem:pip parameters applied. To get a list I needed to use pagination in a way that didn't trigger GitHub's 1000 limit. To do this I need to know how many pages, so I created the code below to dynamically determine the number of pages and store it in the variable \"total_pages\" for the next code block.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c094b6d3-526c-4f93-b719-7050b61d1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE BLOCK #1\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_total_pages_dynamic(base_url, search_query):\n",
    "    \"\"\"\n",
    "    Dynamically finds the highest page number in GitHub advisory results by inspecting the body class for the last 'aria-label' value.\n",
    "\n",
    "    Parameters:\n",
    "        base_url (str): The base URL of the GitHub advisories page.\n",
    "        search_query (str): Search query parameters (e.g., \"type:reviewed ecosystem:pip\").\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of pages found.\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    url = f\"{base_url}?query={search_query}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find the highest page number from the aria-label attribute\n",
    "        pagination_links = soup.find_all('a', {'aria-label': True})\n",
    "        page_numbers = []\n",
    "\n",
    "        for link in pagination_links:\n",
    "            label = link.attrs['aria-label']\n",
    "            if label.startswith(\"Page \"):  # Look for \"Page X\"\n",
    "                try:\n",
    "                    page_number = int(label.replace(\"Page \", \"\").strip())\n",
    "                    page_numbers.append(page_number)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "        if page_numbers:\n",
    "            return max(page_numbers)  # Return the highest page number\n",
    "        else:\n",
    "            print(\"No pagination found. Defaulting to page 1.\")\n",
    "            return 1\n",
    "    else:\n",
    "        print(f\"Failed to fetch page: {response.status_code}\")\n",
    "        return 1\n",
    "\n",
    "# Parameters\n",
    "BASE_URL = \"https://github.com/advisories\"\n",
    "SEARCH_QUERY = \"type:reviewed ecosystem:pip\"\n",
    "\n",
    "# Fetch total pages dynamically\n",
    "total_pages = get_total_pages_dynamic(BASE_URL, SEARCH_QUERY)\n",
    "print(f\"Total pages found: {total_pages}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318f189b-4e44-46c7-a2ed-1cc9449a2bd9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size=4 color=\"#36454F\"><b>Code Block #2: Create a list of advisory URL's for GraphQL API</b></font><br><br>\n",
    "Initially, I didn't see how to \"search\" the GraphQL API applying filters for the type:reviewed ecosystem:pip parameters so I had to try and scrape out a list of URLs for all the type:reviewed ecosystem:pip advisories. The code block below generates the github_advisories_full.csv in which the URL column is used in the next code block.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e25730-2a5b-48d8-ac14-403149a75313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE BLOCK #2\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def get_all_advisories(base_url, search_query, total_pages, delay=2):\n",
    "    \"\"\"\n",
    "    Scrapes GitHub advisories across all pages.\n",
    "\n",
    "    Parameters:\n",
    "        base_url (str): The base URL of the GitHub advisories page.\n",
    "        search_query (str): Search query parameters (e.g., \"type:reviewed ecosystem:pip\").\n",
    "        total_pages (int): Total number of pages to scrape.\n",
    "        delay (int): Delay in seconds between requests to avoid rate limits.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of advisories (dicts).\n",
    "    \"\"\"\n",
    "    advisories = []\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    for page in range(1, total_pages + 1):\n",
    "        print(f\"Fetching page {page}...\")\n",
    "        url = f\"{base_url}?query={search_query}&page={page}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            advisory_elements = soup.find_all('div', class_='Box-row')\n",
    "\n",
    "            for advisory in advisory_elements:\n",
    "                title = advisory.find('a', class_='Link--primary').text.strip()\n",
    "                details_url = advisory.find('a', class_='Link--primary')['href']\n",
    "                severity = advisory.find('span', class_='Label').text.strip() if advisory.find('span', class_='Label') else None\n",
    "                \n",
    "                advisories.append({\n",
    "                    \"Title\": title,\n",
    "                    \"URL\": f\"https://github.com{details_url}\",\n",
    "                    \"Severity\": severity\n",
    "                })\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {page}: {response.status_code}\")\n",
    "        \n",
    "        time.sleep(delay)  # Pause to avoid rate-limiting\n",
    "\n",
    "    return advisories\n",
    "\n",
    "\n",
    "def save_to_csv(advisories, output_file):\n",
    "    \"\"\"\n",
    "    Saves the list of advisories to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        advisories (list): List of advisories (dicts).\n",
    "        output_file (str): Output file path.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(advisories)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Data saved to {output_file}!\")\n",
    "\n",
    "\n",
    "# Parameters\n",
    "BASE_URL = \"https://github.com/advisories\"\n",
    "SEARCH_QUERY = \"type:reviewed ecosystem:pip\"\n",
    "OUTPUT_FILE = \"github_advisories_full.csv\"\n",
    "\n",
    "# Scrape advisories\n",
    "all_advisories = get_all_advisories(BASE_URL, SEARCH_QUERY, total_pages)\n",
    "\n",
    "# Save results\n",
    "save_to_csv(all_advisories, OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c97d51-4a1f-4d53-9aad-20e2eb8bf01c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size=4 color=\"#36454F\"><b>Code Block #3: Download the advisories</b></font><br><br>\n",
    "At the start I was going to download the advisories, as they were, in HTML format. However, in reviewing the graphql API documentation I realized\n",
    "there are only approximately 9 keys available (ghsaId, severity, summary, description, permalink, publisedAt, withdrawnAt, identifiers, and references) so I elected to download the advisories into separate JSON files separated into severity folders.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f41f85c-e90b-4dde-87cc-c8be9e8a0c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE BLOCK #3\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve GitHub token\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "if not GITHUB_TOKEN:\n",
    "    raise Exception(\"GitHub token not found. Please set GITHUB_TOKEN in your .env file.\")\n",
    "\n",
    "# Directory for storing advisories\n",
    "BASE_DIR = \"advisories\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "# Headers for API requests\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {GITHUB_TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Function to extract `ghsaId` from the advisory URL\n",
    "def extract_ghsa_id(url):\n",
    "    return url.split(\"/\")[-1]  # Extract the last part of the URL\n",
    "\n",
    "# Function to fetch advisory details by `ghsaId`\n",
    "def fetch_advisory(ghsa_id):\n",
    "    query = f\"\"\"\n",
    "    query {{\n",
    "      securityAdvisory(ghsaId: \"{ghsa_id}\") {{\n",
    "        ghsaId\n",
    "        severity\n",
    "        summary\n",
    "        description\n",
    "        permalink\n",
    "        publishedAt\n",
    "        withdrawnAt\n",
    "        identifiers {{\n",
    "          type\n",
    "          value\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    response = requests.post(\n",
    "        \"https://api.github.com/graphql\",\n",
    "        json={\"query\": query},\n",
    "        headers=HEADERS\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"data\"][\"securityAdvisory\"]\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch advisory: {response.status_code}, {response.text}\")\n",
    "\n",
    "# Save advisory JSON to the appropriate directory based on severity\n",
    "def save_advisory_by_severity(advisory):\n",
    "    severity = advisory[\"severity\"].lower()\n",
    "    dir_path = os.path.join(BASE_DIR, severity)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(dir_path, f\"{advisory['ghsaId']}.json\")\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(advisory, file, indent=4)\n",
    "    print(f\"Saved: {file_path}\")\n",
    "\n",
    "# Main function to process advisory URLs from CSV\n",
    "def process_advisories_from_csv(csv_file):\n",
    "    with open(csv_file, \"r\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            url = row[\"URL\"]  # Adjust column name if needed\n",
    "            try:\n",
    "                ghsa_id = extract_ghsa_id(url)\n",
    "                advisory = fetch_advisory(ghsa_id)\n",
    "                save_advisory_by_severity(advisory)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "# Path to the CSV file containing advisory URLs\n",
    "csv_file_path = \"github_advisories_full.csv\"  # Update with your actual file path\n",
    "\n",
    "# Process advisories from the CSV file\n",
    "process_advisories_from_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79ed01a-5f19-4301-93b7-91319ed2e7cb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size=4 color=\"#36454F\"><b>Code Block #4: Merge all the JSON advisories into one to correlate with CISA KVE</b></font><br><br>\n",
    "All of the Advisory JSON files in the /advisories/severity folders are merge into one JSON to make it easier to correlate with CISA JSON.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39b902b-7ea4-4e62-bbd6-2e4399769ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE BLOCK #4\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Define the base advisories directory and output file\n",
    "BASE_DIR = \"advisories\"\n",
    "OUTPUT_FILE = os.path.join(BASE_DIR, \"advisories_all.json\")\n",
    "\n",
    "# Subdirectories containing advisories categorized by severity\n",
    "SEVERITY_DIRS = [\"low\", \"moderate\", \"high\", \"critical\"]\n",
    "\n",
    "def merge_json_files(base_dir, severity_dirs, output_file):\n",
    "    all_advisories = []  # List to store all advisories\n",
    "\n",
    "    # Loop through each severity directory\n",
    "    for severity in severity_dirs:\n",
    "        severity_dir = os.path.join(base_dir, severity)\n",
    "\n",
    "        if not os.path.exists(severity_dir):\n",
    "            print(f\"Directory not found: {severity_dir}\")\n",
    "            continue\n",
    "\n",
    "        # Loop through all JSON files in the severity directory\n",
    "        for file_name in os.listdir(severity_dir):\n",
    "            if file_name.endswith(\".json\"):\n",
    "                file_path = os.path.join(severity_dir, file_name)\n",
    "                \n",
    "                # Read and append the JSON content to the list\n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "                        advisory = json.load(json_file)\n",
    "                        all_advisories.append(advisory)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "    # Write all advisories to the output JSON file\n",
    "    try:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as output_json:\n",
    "            json.dump(all_advisories, output_json, indent=4)\n",
    "        print(f\"All advisories merged and saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to {output_file}: {e}\")\n",
    "\n",
    "# Run the function to merge JSON files\n",
    "merge_json_files(BASE_DIR, SEVERITY_DIRS, OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48f6f26-013d-48b2-b98b-3a32f4238085",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size=4 color=\"#36454F\"><b>Code Block #4: Download the CISA JSON File</b></font><br><br>\n",
    "Download the CISA KVE as a JSON file.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26011be-be5e-4cd8-ba5a-45487b903485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the JSON file\n",
    "url = \"https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json\"\n",
    "\n",
    "# Path to save the downloaded JSON file\n",
    "output_file = \"known_exploited_vulnerabilities.json\"\n",
    "\n",
    "def download_json(url, output_file):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        \n",
    "        # Save the content to a local file\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(response.text)\n",
    "        \n",
    "        print(f\"JSON file successfully downloaded and saved as '{output_file}'.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download the JSON file: {e}\")\n",
    "\n",
    "# Call the function to download the JSON\n",
    "download_json(url, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87dc975-0a6f-46a6-9de6-e23b8e0e62e0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size=4 color=\"#36454F\"><b>Code Block #5: Create the extra credit CSV</b></font><br><br>\n",
    "This code block compares the advisories advisories_all.json file identfiers type CVE value with the cveID value in the known_exploited_vulnerabilities.json and if there is a match it sets a value of 1 in the corresponding KVE column, if not the cell value is\n",
    "NULL. For key information I used the values of \"GHSAID\", \"SEVERITY\", \"SUMMARY\", \"DESCRIPTION\", \"PUBLISHED\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424eb212-2062-4335-843f-bc65cb86fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE BLOCK #5\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Load JSON data\n",
    "with open('advisories_all.json', 'r') as f:\n",
    "    advisories = json.load(f)\n",
    "\n",
    "with open('known_exploited_vulnerabilities.json', 'r') as f:\n",
    "    kev_data = json.load(f)\n",
    "\n",
    "# Create a set of all CVE IDs in KEV catalog\n",
    "kev_cve_ids = {vuln[\"cveID\"] for vuln in kev_data[\"vulnerabilities\"]}\n",
    "\n",
    "# Extract advisory info with KEV comparison\n",
    "rows = []\n",
    "for advisory in advisories:\n",
    "    ghsa_id = advisory.get(\"ghsaId\")\n",
    "    severity = advisory.get(\"severity\")\n",
    "    summary = advisory.get(\"summary\")\n",
    "    published = advisory.get(\"publishedAt\")\n",
    "    \n",
    "    # Find CVE identifiers\n",
    "    cve_ids = [id_[\"value\"] for id_ in advisory.get(\"identifiers\", []) if id_[\"type\"] == \"CVE\"]\n",
    "    kev_flag = 1 if any(cve in kev_cve_ids for cve in cve_ids) else None\n",
    "\n",
    "    rows.append({\n",
    "        \"GHSAID\": ghsa_id,\n",
    "        \"SEVERITY\": severity,\n",
    "        \"SUMMARY\": summary,\n",
    "        \"PUBLISHED\": published,\n",
    "        \"KEV\": kev_flag\n",
    "    })\n",
    "\n",
    "# Write to CSV\n",
    "output_file = \"advisory-kev-correlation.csv\"\n",
    "with open(output_file, \"w\", newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"GHSAID\", \"SEVERITY\", \"SUMMARY\", \"PUBLISHED\", \"KEV\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"CSV file created: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4271810-0c75-4b97-8d41-40a07f472763",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size=4 color=\"#36454F\"><b>Code Block #6: Zip it!</b></font><br><br>\n",
    "Create a zip file by severity as requested. The zip file will be in the /advisories/severity folders.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a0fc038-3b7f-4152-a311-662444004508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All ZIP files created successfully.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Advisory levels\n",
    "levels = ['critical', 'high', 'moderate', 'low']\n",
    "\n",
    "for level in levels:\n",
    "    folder_path = os.path.join('advisories', level)\n",
    "    zip_path = os.path.join(folder_path, f'{level}.zip')\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file != f'{level}.zip':  # Avoid recursion\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, folder_path)\n",
    "                    zipf.write(file_path, arcname)\n",
    "\n",
    "print(\"All ZIP files created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb76e55-e849-4e20-b675-f60a66001f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
